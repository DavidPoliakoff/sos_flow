\todofilebegin{030\_sos\_model.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Design and Architecture of SOS}

\subsection{The Need to Observe}
Because the features we are interested in are going to emerge at run
time, it is necessary to obtain observations of real-world
applications, libraries, data transmissions, and hardware state. To
realize a model for performance that is practically suited for
exascale architecures, it is further necessary to collect and analyze
information as close as possible to its source, and to accept metrics
submitted from an arbitrary array of sources.

This in situ runtime design
effectively parallelizes the execution of the observation
system and gives opportunities to cut down on coordination
overhead when providing mechanisms for on-line feedback and control.


%-----------------------------------------------------------------------------

\subsection{What is Observed}

Many factors have contributed to the emergence of variability studies
as an important research topic within the HPC community. At a
low-level, HPC node engineering has grown in complexity and
sophistication, many on-core processor behaviors that used to be
isolated, synchronous, and predictable, are now interrelated,
data-driven, asynchronous, and impossible to predict a priori. As core
density increases on the nodes of a cluster the unpredictability and
inconsistency of the hardware itself becomes an increasingly
significant contributor to observed variability. Hardware is an
important source to consider when it comes to variability because
there is almost nothing that can be done to control for it, the noise
has to be admitted in the results, and therefore is an important part
of the output of any performance-related experiment.

Performance of HPC codes can be impacted from many different sources:
\begin{itemize}
    \item Versions of software libraries across can differ across
      clusters, or even the same cluster across time.
    \item Tuning factors to extract maximum performance from a code
      can vary across clusters even if the code and the data do not
      change. Shared filesystem performance, data transport methods,
      available per-process memory, co-processor presence and
      architecture, ...and more, all can be responsible for
      influencing sensitivity to novel tuning factors.
    \item Concurrent activity elsewhere on the same cluster, activity
      that will necessarily vary between every iteration of the
      workflow, may be having a significant impact on observed
      performance.
    \item At extreme scales, parts of a simulation are almost
      guaranteed to fail due to the marginal failure rates of hardware
      components approaching absolute certainty as the number of
      involved components increases. These failures cannot be
      accurately predicted a priori, and the design constraints that
      account for and respond to them introduce performance
      perturbation and further complexity.
\end{itemize}


%-----------------------------------------------------------------------------

\subsection{How to Observe}
SOS has been designed as a voluntary-participation runtime daemon
offering a programmable middleware layer allowing adaptivity in metric
expression, combination, feedback, and response.

\subsubsection{``Put the Data Somewhere Useful''}
Because of the variety of metrics of interest, the fact that they may
not always be expressed in any given runtime, and that all possible
metrics are not known a priori to be hardcoded into a system, it is
imperative from an architectural point of view that performance
observations are gatherd in common from all sources.  Some programm
spinning snapshots of its internal state out into a proprietary
database is denying all other parts of the environment the ability to
learn more about each other, themselves, and that program.

\subsubsection{Simple, Clean, Universal}
Rather than seeing yet another printf()-style instrumentation
dead-end, a well-designed observation system will provide a clean and
flexible interface for suitable for capturing and annotating metrics
within their context, and will bring the metrics from all sources
together to be operated over following a shared set of semantics.

\subsubsection{Observation vs. Action}
The need to observe, what to observe, and how to observe it are
individually and collectively decoupled from the programming logic of
any given source of data. Where the observation system comes back and
interfaces with the application logic is in the full fuition of the
SOS architecture, where applications express sensitivity to various
triggers, and independent analytics agents are able to fire those triggers
based on looking at the wholistic store of data.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{SOS: Scalable Observation System}

SOS is concieved of as an architectural addition to the general fabric of
exascale clusters. It is intended to be a deeply-integrated service with low-level
optimizations and a set of functionality that is available to all users of an exascale
cluster.

%-----------------------------------------------------------------------------

\subsection{SOSflow}
Applying the semantic workflow performance model to actual
run-time environments necessitated the development of new
infrastructure software, and so the second contribution to the general
challenges of monitoring scientific workflows is the SOSflow middleware.
SOSflow is designed to:

\begin{enumerate}
\item Facilitate on-line capture of performance data and various system
  events in situ (i.e. ``on node'') from arbitrary sources and
  perspectives at the same time.
\item Annotate the gathered data's semantic meaning with expressive
  ``high-level'' tags that facilitate contextualization and
  introspective analytics.
\item Store the captured data on node in a way that can be searched
  with dynamic queries in real-time as well as being suitable for
  long-term centralized archival.
\end{enumerate}

SOSflow is divided into two main parts in it's current incarnation:
\begin{itemize}
\item sosd - Daemon process running on each node
\item libsos - Library of routines for interacting with the daemon
\end{itemize}

\subsubsection{Behavior}
The sosd daemon launches before a scientific workflow begins, and
passively listens on a socket.  The port that is available to all of
the sources on any given node is found in the "SOS\_CMD\_PORT"
environment variable.  Many programs and layers of programs can
connect to the daemon and send information in. Library-to-daemon
communication is invisible to the SOSflow user and happens entirely
on-node using a simple stateless protocol.  When the workflow is
completed, the sosd\_stop tool is executed on each node, it signals
the daemon to flush buffers and close down.



\subsubsection{Implementation}
\begin{itemize}
  \item \textbf{Language}: C99
   \item \textbf{External Requirements}:
   \begin{itemize}
    \item cmake
    \item pthreads
    \item MPI
    \item Sqlite3
    \end{itemize}
  \item \textbf{Key Source Files}: See Table 1.
\end{itemize}

\begin{table}[!t]
%% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{Key Source Files for SOSflow}
\label{table_example}
\centering
\begin{tabular}{|c|c|}
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
sos.h / sos.c & Becomes libsos, the core functions of SOSflow\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
sosd.h / sosd.c & SOSflow daemon\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
sosa.h / sosa.c & SOS analytics core utilities\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
sos\_debug.h & Debugging off / on (level) knobs\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
demo\_app.c & The "Hello, world." of SOSflow value injection\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
sosd\_cloud\_mpi.c & Off-node transport using simple MPI calls\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
sosd\_db\_sqlite.c & On-node DB creation and value-injection\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{tabular}
\end{table}

%-----------------------------------------------------------------------------

\subsection{Limitations and Concerns}
SOSflow is a programmable middleware platform.  The value of SOSflow is
directly proportional to the quantity and quality of the sources that
are pushing semantically-annotated data into it.

The more sources that are instrumented with SOSflow the less limited
the workflow performance model will be. When only one or two layers
are instrumented, the benefits of the semantic annotation and the
workflow performance model are naturally limited.

The SOSflow software artifact is in its initial development cycle and
has room for improvement in various software engineering facets:
memory subsystem interaction, diversity of storage and transport
mechanisms, and quality-of-service guarantees to both producers and
consumers of SOSflow data.

%-----------------------------------------------------------------------------

\subsection{SOSanalytics}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Semantic Performance Model}
Traditionally, HPC performance monitoring is focused on low-level
efficiency of an application or library (TAU) on some particular mechine.
Higher-level systems (TACC Stats) allow the tracking and exploration
of execution wall-time for various library versions, integration of
multiple modalities of information (LDMS) like program invocation or
work allocation across a cluster as informed by network congestion
statistics, and other hybridized or meta-execution data points.
Low-level metrics are more naturally suited for off-line episodic
performance analysis of individual workflow components, but cannot
yield insight into the run-time performance of a complex workflow.
Characterizing and understanding the emergent properties of a workflow
comprised of many components that are interacting asynchronously
across a distributed HPC cluster requires taking a new approach to the
problem, especially when considering the extreme scales of parallelism
to which scientific workflows are being driven.

%-----------------------------------------------------------------------------

\subsection{Invariant Meaning}

%Figures are revealed by their background, as they break up the continuity
%of the background they stand-out in the world. By defining an invariant
%background for performance variation, the meaning of the codes, the
%...metaphor breaks down.  It's really about mapping disanalogous sets of
%metrics over each other according to a set of organizing principles that
%are the invariant for that computation.

So long as the input data and result output is the same, what never
changes about a simulation, nomatter how it is tuned or allocated or
configured for a run, is the meaning of what is happening.  Whether a
simulation is executing as a single process on a single node, or
billions of independent tasks in a cloud computing environment, the
meaning of what is being computed holds the same.

At extreme scales, on line detection and attribution of variability of
a scientific workflow will require well-annotated metadata to
facilitate "apples to apples" comparisons driven by unsupervised
machine learning rather than a priori developer knowledge or offline
centralized analysis. This kind of semantic invariance, in the face of
wild performance variance, is important to characterize a code's
sensitivity to performance perturbation, as well as a cluster's
propensity for creating and propagating performance variability.

%-----------------------------------------------------------------------------

\subsection{Levels of Description and "The View from Anywhere"}
Not knowing \textit{a priori} what component or layer of the workflow
will be interesting for the analysis of performance, the workflow
performance model needs to be populated by a diversity of information
sources that provide metrics with tailored metadata and both logical
and concrete events, arriving in real-time from many different layers
of activity in a workflow:
\begin{itemize}
    \item Workflow
    \item Simulation
    \item Application
    \item Algorithm
    \item Libraries
    \item Environment
    \item Developer Tools
    \item Operating System
    \item Node Hardware
    \item Network
    \item Enclave
    \item Cluster
    \item Epoch	
\end{itemize}

Each of these layers constitutes a level of description for the
overall system performance, and the performance of any component is a
composite of slices through any layers that component is participating
in or contingent on. Metrics from multiple layers can be correlating
to yield interesting perspectives on the observed performance of
specific software components.  For example, something at the
Simulation layer produces data representing the evolution of a data
set over N seconds of simulated time, and the Application layer
requires M seconds of real-world computation to yield that data, the
relationship between N and M could be a valid performance metric to
report, compare across runs, calculate real-dollar-cost to compute, or
attempt to generally optimize through parameter convergence. \textit{The
particular purpose for gathering that data would NOT need to be known
in advance}, the sampling would not need to be programmed into the
application by hand, rather, on-demand queries could be run to
discover such correlations and features of performance at any time, or
\textit{even in real-time}. Importantly, components of a layer not
only can contribute metrics, they (and other components at that layer)
are targets for feedback and control.

%-----------------------------------------------------------------------------

\subsection{Semantics}
All information that is gathered by the monitoring system should be
annotated as richly as possible to maximize its usefulness when
performing analytics.  Hand-annotated codes will have the most to
offer an analytics engine, values that are tracked will be able to
carry a full spectrum of high-level tags that express what that data
point means and what could be expected of it, in structure preserving
a human programmer or user's understanding while also being compatible
with unsupervised machine-learning tests for significance and other
advanced analysis techniques.

Any episodic performance measurement, such as run-time TAU
instrumentation, can also be injected into the SOSflow engine, and the
SOSflow runtime will be able to differentiate from information pushed
directly by a layer, and information that is being captured by
middle-ware tools.

\todo[inline]{Insert table heres of some of the enum values alongside
  plain-english descriptions.}

Semantic information is local to the "publication handle" (pub)
created by a source that is contributing to SOSflow.  Sources can
create multiple pubs to distinctly represent potential compound or
complex roles. Pubs carry their own pub-wide semantic markups,
including the origin layer, a role within that layer, and information
about the node that the source process is running on. Semantic markups
are then nested inside of the pub handle, as each value that is pushed
into the SOSflow system through a pub handle also comes with a rich
set of high-level semantic tags that stay affiliated with it over
time. While deep off-line data analytics can reveal unforseen
correlations between various aspects of the workflow or the data set
it is operating over, the interest in real-time analytics and
performance tuning gives value to expressing "relationship hints"
between values tracked by the system. These hints can be used to
direct in situ analytics and identify deviations from expectations;
anything that can be overtly identified as an expectation for a value
can be used to narrow the search space when doing unsupervised machine
learning over gathered workflow performance data.

%-----------------------------------------------------------------------------

\subsection{The Utility a Comprehensive Semantic Performance Model}
\begin{itemize}
\item \textbf{Attribution}: Point the blame at the offending job or shared
  resource. Also, developers don't always know their own code so well,
  or how it will interact with a total system, so capturing from a
  wide array of sources will help eliminate the guessing game.
\item \textbf{Accuracy}:
\item \textbf{Resource Requirement Prediction}:
\item \textbf{Automated Component Performance Tuning}: Don't want conflicting
  optimizer purposes. Need to know where the hotspots really are and
  not depend on individual developers being total system experts.
\item \textbf{Intelligent Compiler Hints}: Don't have to burn 1,000,000 hours
  of allocation to learn that you just re-created last year's mistaken
design choices.
\item \textbf{Intelligent Job Scheduling}:
\end{itemize}

\todofileend{030\_sos\_model.tex}
