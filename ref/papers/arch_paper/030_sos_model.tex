\todofilebegin{030\_sos\_model.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Design and Architecture of SOS}

\subsection{The Need to Observe}
Because the features we are interested in are going to emerge at run
time, it is necessary to obtain observations of real-world
applications, libraries, data transmissions, and hardware state. To
realize a model for performance that is practically suited for
exascale architecures, it is further necessary to collect and analyze
information as close as possible to its source, and to accept metrics
submitted from an arbitrary array of sources.

This in situ runtime design
effectively parallelizes the execution of the observation
system and gives opportunities to cut down on coordination
overhead when providing mechanisms for on-line feedback and control.


%-----------------------------------------------------------------------------

\subsection{What is Observed}

Many factors have contributed to the emergence of variability studies
as an important research topic within the HPC community. At a
low-level, HPC node engineering has grown in complexity and
sophistication, many on-core processor behaviors that used to be
isolated, synchronous, and predictable, are now interrelated,
data-driven, asynchronous, and impossible to predict a priori. As core
density increases on the nodes of a cluster the unpredictability and
inconsistency of the hardware itself becomes an increasingly
significant contributor to observed variability. Hardware is an
important source to consider when it comes to variability because
there is almost nothing that can be done to control for it, the noise
has to be admitted in the results, and therefore is an important part
of the output of any performance-related experiment.

Performance of HPC codes can be impacted from many different sources:
\begin{itemize}
    \item Versions of software libraries across can differ across
      clusters, or even the same cluster across time.
    \item Tuning factors to extract maximum performance from a code
      can vary across clusters even if the code and the data do not
      change. Shared filesystem performance, data transport methods,
      available per-process memory, co-processor presence and
      architecture, ...and more, all can be responsible for
      influencing sensitivity to novel tuning factors.
    \item Concurrent activity elsewhere on the same cluster, activity
      that will necessarily vary between every iteration of the
      workflow, may be having a significant impact on observed
      performance.
    \item At extreme scales, parts of a simulation are almost
      guaranteed to fail due to the marginal failure rates of hardware
      components approaching absolute certainty as the number of
      involved components increases. These failures cannot be
      accurately predicted a priori, and the design constraints that
      account for and respond to them introduce performance
      perturbation and further complexity.
\end{itemize}


%-----------------------------------------------------------------------------

\subsection{How to Observe}
SOS has been designed as a voluntary-participation runtime daemon
offering a programmable middleware layer allowing adaptivity in metric
expression, combination, feedback, and response.

\subsubsection{``Put the Data Somewhere Useful''}
Because of the variety of metrics of interest, the fact that they may
not always be expressed in any given runtime, and that all possible
metrics are not known a priori to be hardcoded into a system, it is
imperative from an architectural point of view that performance
observations are gatherd in common from all sources.  Some programm
spinning snapshots of its internal state out into a proprietary
database is denying all other parts of the environment the ability to
learn more about each other, themselves, and that program.

\subsubsection{Simple, Clean, Universal}
Rather than seeing yet another printf()-style instrumentation
dead-end, a well-designed observation system will provide a clean and
flexible interface for suitable for capturing and annotating metrics
within their context, and will bring the metrics from all sources
together to be operated over following a shared set of semantics.

\subsubsection{Observation vs. Action}
The need to observe, what to observe, and how to observe it are
individually and collectively decoupled from the programming logic of
any given source of data. Where the observation system comes back and
interfaces with the application logic is in the full fuition of the
SOS architecture, where applications express sensitivity to various
triggers, and independent analytics agents are able to fire those triggers
based on looking at the wholistic store of data.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{SOS: Scalable Observation System}

SOS is concieved of as an architectural addition to the general fabric of
exascale clusters. It is intended to be a deeply-integrated service with low-level
optimizations and a set of functionality that is available to all users of an exascale
cluster.

%-----------------------------------------------------------------------------

\subsection{SOSflow}
Applying the semantic workflow performance model to actual
run-time environments necessitated the development of new
infrastructure software, and so the second contribution to the general
challenges of monitoring scientific workflows is the SOSflow middleware.
SOSflow is designed to:

\begin{enumerate}
\item Facilitate on-line capture of performance data and various system
  events in situ (i.e. ``on node'') from arbitrary sources and
  perspectives at the same time.
\item Annotate the gathered data's semantic meaning with expressive
  ``high-level'' tags that facilitate contextualization and
  introspective analytics.
\item Store the captured data on node in a way that can be searched
  with dynamic queries in real-time as well as being suitable for
  long-term centralized archival.
\end{enumerate}

SOSflow is divided into several components.  The core components are:
\begin{itemize}
\item libsos - Library of common routines for interacting with
      sosd daemons and SOS data structures
\item sosd(listener) - Daemon process running on each node
\item sosd(db) - Daemon process running on dedicated resources
      that stores data aggregated from one or more in situ daemons
\item sosa - Analytics framework for online query of SOS data
\end{itemize}

Some accessory components are:
\begin{itemize}
\item sosd\_probe - Collect real-time statistics from the daemon about
      its internal state and operating workload
\item sosd\_stop - Signal the SOSflow runtime to store its queues and
      safely shut down
\item demo\_app - Benchmarking tool for testing SOSflow
\item SOApy - Python scripts for querying the SOS databases and plotting
      graphs from the results
\item synthetic\_flow - Tool for generating SOSflow-instrumented
      synthetic workflow of components using ADIOS+FlexPath
      that self-assemble into an arbitrary  user-specified DAG of
      data-dependent processing relations.
\end{itemize}

\subsubsection{Behavior}
The sosd daemon is itself an MPI application, and it is launched as a
background process in user space at the start of a job script, before
the scientific workflow begins.  The daemon first goes through a
coordination phase where they performs an MPI\_Allreduce() with all
other daemon ranks in order to share their role (DAEMON, DB, or
ANALYTICS) and the name of the host they are running on. During the
coordination phase, listener daemons select the sosd(db) aggregate
database that they will target for automatic asynchronous transfer of
the data they capture. At present, the ranks of sosd(listener) use a
simple round-robin selection method to pick their database target. All
the sosd instances split up the total numeric space from 0 to
UINT64\_MAX for use in assigning globally unique ids (GUIDs). All
entries in the SOSflow system from that point forward are, if
appropriate, assigned a GUID such that uniqueness and provenance is
preserved even if queries straddle multiple data stores, or all data
stores are eventually synthesized into a global store.

Analytics modules use this time to determine if they are co-located
with a sosd(db) role, and set an internal flag that allows analytics
developers to branch their logic (if desired) between local instances that
can more quickly coordinate queries with the aggregate database, and any 
other optional independent ranks that are running on dedicated resources
for the purpose of doing calculations or rendering visualizations. After
having coordinate with the sosd ranks, sosa ranks split off into their own
private communicator, allowing the developer of a given sosa module to
utilize the collective communication benefits of the HPC platform they
are deployed on.

Once all of the sosd and sosa ranks are initialized and aware of each
other, they begin operating autonomously and no longer exchange global
collective messages -- all MPI messages are now point-to-point. The
sosd(listener) in situ daemons go into a loop listening to a
node-local TCP socket (specified by the ``SOS\_CMD\_PORT'' environment
variable) for messages from utilities or SOSflow-enabled source
applications.  sosd(db) database daemons do not monitor any socket at
this time, but rather are contacted exclusively through their MPI
communicator. When an analytics module submits a query, it does not
address the database files directly, but sends its query to the
sosd(db) instance over MPI\_COMM\_WORLD and receives results back as
an MPI message which is transparently parsed into a useful ODBC-like
``RecordSet'' style object.

On node, the sosd(listener) program accepts single messages at a time
and immediately services them, as far as the message sender is aware,
quickly responding with a simple ACK message or with the data that was
requested.  This behavior follows a very simple stateless protocol
that exists between the daemon and its on-node clients. Messages are
read off the socket rapidly and placed in the daemon's unbounded
asynchronous thread-safe queues to be processed by the daemon's
local\_sync and cloud\_sync threads. The socket is nearly immediately
clear for the next queue'd message to be received. Communication with
the sosd(listener) is always initiated by the clients.  Client
applications do not need to monitor a socket, though certain SOS
client roles can spawn a lightweight background thread that
periodically checks in with their local daemon to see if any feedback
has been sent to that application, allowing for run-time adaptivity to
be decoupled from the application's schedule for transmitting its
information to SOSflow.

All of the communication functions in the sos client library are
handled transparently, SOS users need only interact with a simple API
to define and store values that they can then publish up to the daemon
as appropriate.  The protocols and the codes of the client library are
designed to be fast and minimize resource usage, though they will
buffer values for the user if they choose to hold them and only
transmit to the daemon at intervals.  It is very uncommon to see the
blocking socket transmission calls within SOS\_announce() and
SOS\_publish() take longer than 0.002 seconds in any of our testing
scenarios. Calls to the more common SOS\_pack() value storage/update
routine create no significant overhead at all when not used in the
innermost loops of intense computation, often returning in less than
0.0001 seconds even for very large data sets. When a client first
registers with the sosd(listener) during SOS\_init(), the daemon will
assign that client library a break of GUIDs to manage internally, so
that many different client activities can be handled correctly with no
need for immediate interaction with the daemon. When a client library
runs out of GUIDs it simply pings the daemon and asks for more.

The typical order of API calls in a client is:
\begin{enumerate}
    \item SOS\_init();
    \item SOS\_pub\_create();
    \item SOS\_pack();
    \item SOS\_announce();
    \item SOS\_publish();\\
    \dots
    \item SOS\_finalize();
\end{enumerate}

Data from clients in SOSflow is stored in a SOS\_pub * ``pub handle''
object. This object organizes all of the application context
information and value-specific metadata, as well as managing the
history of updates to a value pending pending transmission to a
sosd(listener), called \textit{value snapshots}. Every value that is
passed through the SOSflow API is preserved and eventually stored in a
searchable database, along with any updated metadata such as its
timestamp tuples.  Multiple updates to the same value prior to a call
to SOS\_publish() are no exception, prior value snapshots are queued
up and faithfully transmitted along with the most recent update to
that value.

SOSflow is not a pure publish/subscribe system, the SOS\_announce()
function and its associated message type exist (for now) to seperate the
transmission of metadata apart from the transmission of the typically
more compact values stored in a pub.  Calling the SOS\_announce()
function is entirely optional, though never harmful, and good
developer hygene in case it is not optional at some point in the
future. If a pub handle has not been announced when it is passed to
the publish function, it with automatically recurse into the announce
function on your behalf.  Additionally, if you have added new value
names, and not merely packed updates to some previously announced
values, the next time you publish, those new value names will
automatically be announced.

SOSflow is entirely thread-safe within the client application so long
as data elements are accessed only through the public API. You could,
if you for some reason wanted to, share a pub handle between two
threads: the system is designed to efficiently shield users from the
complexity of managing its state.  It is also designed to be as simple
as possible without losing flexibility. When a value is packed into a
pub handle, for the first time, that effectively defines the value and
configures it with a totally safe set of metadata defaults. If inspired
to, a developer using SOSflow could then go render the metadata more
precise, though such noble pursuits are entirely optional. Data is
thus defined to the system the same way it is updated in the system,
through the SOS\_pack() function.

The in situ sosd(listener) daemon does not keep track of applications,
and so does not know (or care) when they terminate. The call to SOS\_finalize()
provides a simple and clean way to clean up SOS's internal memory objects
and buffers, join any threads it may have needed to spawn, and otherwise
put things back how it found them when SOS\_init() was called. In the
near future it is expected that SOS\_finalize() \textbf{will} message the
daemon, and so client applications should always make an effort to call
this function explicitly.


\subsubsection{SOSflow Implementation}
\begin{itemize}
  \item \textbf{Language}: C99
   \item \textbf{External Requirements}:
   \begin{itemize}
    \item cmake
    \item pthreads
    \item MPI
    \item Sqlite3
    \end{itemize}
  \item \textbf{Key Source Files}: See Table 1.
\end{itemize}

\begin{table}[!t]
%% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{Key Source Files for SOSflow}
\label{tableexample}
\centering
\begin{tabular}{|c|c|}
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
sos.h / sos.c & Becomes libsos, the core functions of SOSflow\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
sosd.h / sosd.c & SOSflow daemon\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
sosa.h / sosa.c & SOS analytics core utilities\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
sos\_debug.h & Debugging off / on (level) knobs\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
demo\_app.c & The "Hello, world." of SOSflow value injection\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
sosd\_cloud\_mpi.c & Off-node transport using simple MPI calls\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
sosd\_db\_sqlite.c & On-node DB creation and value-injection\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{tabular}
\end{table}

%-----------------------------------------------------------------------------

\subsection{Limitations and Concerns}
SOSflow is a programmable middleware platform.  The value of SOSflow is
directly proportional to the quantity and quality of the sources that
are pushing semantically-annotated data into it.

The more sources that are instrumented with SOSflow the less limited
the workflow performance model will be. When only one or two layers
are instrumented, the benefits of the semantic annotation and the
workflow performance model are naturally limited.

The SOSflow software artifact is in its initial development cycle and
has room for improvement in various software engineering facets:
memory subsystem interaction, diversity of storage and transport
mechanisms, and quality-of-service guarantees to both producers and
consumers of SOSflow data.

%-----------------------------------------------------------------------------

\subsection{SOSanalytics}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Semantic Performance Model}
Traditionally, HPC performance monitoring is focused on low-level
efficiency of an application or library (TAU) on some particular mechine.
Higher-level systems (TACC Stats) allow the tracking and exploration
of execution wall-time for various library versions, integration of
multiple modalities of information (LDMS) like program invocation or
work allocation across a cluster as informed by network congestion
statistics, and other hybridized or meta-execution data points.
Low-level metrics are more naturally suited for off-line episodic
performance analysis of individual workflow components, but cannot
yield insight into the run-time performance of a complex workflow.
Characterizing and understanding the emergent properties of a workflow
comprised of many components that are interacting asynchronously
across a distributed HPC cluster requires taking a new approach to the
problem, especially when considering the extreme scales of parallelism
to which scientific workflows are being driven.

%-----------------------------------------------------------------------------

\subsection{Invariant Meaning}

%Figures are revealed by their background, as they break up the continuity
%of the background they stand-out in the world. By defining an invariant
%background for performance variation, the meaning of the codes, the
%...metaphor breaks down.  It's really about mapping disanalogous sets of
%metrics over each other according to a set of organizing principles that
%are the invariant for that computation.

So long as the input data and result output is the same, what never
changes about a simulation, nomatter how it is tuned or allocated or
configured for a run, is the meaning of what is happening.  Whether a
simulation is executing as a single process on a single node, or
billions of independent tasks in a cloud computing environment, the
meaning of what is being computed holds the same.

At extreme scales, on line detection and attribution of variability of
a scientific workflow will require well-annotated metadata to
facilitate "apples to apples" comparisons driven by unsupervised
machine learning rather than a priori developer knowledge or offline
centralized analysis. This kind of semantic invariance, in the face of
wild performance variance, is important to characterize a code's
sensitivity to performance perturbation, as well as a cluster's
propensity for creating and propagating performance variability.

%-----------------------------------------------------------------------------

\subsection{Levels of Description and "The View from Anywhere"}
Not knowing \textit{a priori} what component or layer of the workflow
will be interesting for the analysis of performance, the workflow
performance model needs to be populated by a diversity of information
sources that provide metrics with tailored metadata and both logical
and concrete events, arriving in real-time from many different layers
of activity in a workflow:
\begin{itemize}
    \item Workflow
    \item Simulation
    \item Application
    \item Algorithm
    \item Libraries
    \item Environment
    \item Developer Tools
    \item Operating System
    \item Node Hardware
    \item Network
    \item Enclave
    \item Cluster
    \item Epoch	
\end{itemize}

Each of these layers constitutes a level of description for the
overall system performance, and the performance of any component is a
composite of slices through any layers that component is participating
in or contingent on. Metrics from multiple layers can be correlating
to yield interesting perspectives on the observed performance of
specific software components.  For example, something at the
Simulation layer produces data representing the evolution of a data
set over N seconds of simulated time, and the Application layer
requires M seconds of real-world computation to yield that data, the
relationship between N and M could be a valid performance metric to
report, compare across runs, calculate real-dollar-cost to compute, or
attempt to generally optimize through parameter convergence. \textit{The
particular purpose for gathering that data would NOT need to be known
in advance}, the sampling would not need to be programmed into the
application by hand, rather, on-demand queries could be run to
discover such correlations and features of performance at any time, or
\textit{even in real-time}. Importantly, components of a layer not
only can contribute metrics, they (and other components at that layer)
are targets for feedback and control.

%-----------------------------------------------------------------------------

\subsection{Semantics}
All information that is gathered by the monitoring system should be
annotated as richly as possible to maximize its usefulness when
performing analytics.  Hand-annotated codes will have the most to
offer an analytics engine, values that are tracked will be able to
carry a full spectrum of high-level tags that express what that data
point means and what could be expected of it, in structure preserving
a human programmer or user's understanding while also being compatible
with unsupervised machine-learning tests for significance and other
advanced analysis techniques.

Any episodic performance measurement, such as run-time TAU
instrumentation, can also be injected into the SOSflow engine, and the
SOSflow runtime will be able to differentiate from information pushed
directly by a layer, and information that is being captured by
middle-ware tools.

\todo[inline]{Insert table heres of some of the enum values alongside
  plain-english descriptions.}

Semantic information is local to the "publication handle" (pub)
created by a source that is contributing to SOSflow.  Sources can
create multiple pubs to distinctly represent potential compound or
complex roles. Pubs carry their own pub-wide semantic markups,
including the origin layer, a role within that layer, and information
about the node that the source process is running on. Semantic markups
are then nested inside of the pub handle, as each value that is pushed
into the SOSflow system through a pub handle also comes with a rich
set of high-level semantic tags that stay affiliated with it over
time. While deep off-line data analytics can reveal unforseen
correlations between various aspects of the workflow or the data set
it is operating over, the interest in real-time analytics and
performance tuning gives value to expressing "relationship hints"
between values tracked by the system. These hints can be used to
direct in situ analytics and identify deviations from expectations;
anything that can be overtly identified as an expectation for a value
can be used to narrow the search space when doing unsupervised machine
learning over gathered workflow performance data.

%-----------------------------------------------------------------------------

\subsection{The Utility a Comprehensive Semantic Performance Model}
\begin{itemize}
\item \textbf{Attribution}: Point the blame at the offending job or shared
  resource. Also, developers don't always know their own code so well,
  or how it will interact with a total system, so capturing from a
  wide array of sources will help eliminate the guessing game.
\item \textbf{Accuracy}:
\item \textbf{Resource Requirement Prediction}:
\item \textbf{Automated Component Performance Tuning}: Don't want conflicting
  optimizer purposes. Need to know where the hotspots really are and
  not depend on individual developers being total system experts.
\item \textbf{Intelligent Compiler Hints}: Don't have to burn 1,000,000 hours
  of allocation to learn that you just re-created last year's mistaken
design choices.
\item \textbf{Intelligent Job Scheduling}:
\end{itemize}

\todofileend{030\_sos\_model.tex}
