
\section{The Need to Observe}
\todo[inline]{explain terms: cores, nodes, loosely-coupled, etc.}
\todo[inline]{set up the necessity for a new performance model....  }
The designs of SOS and its semantic performance model reflect many of
the new challenges introduced by exascale HPC clusters, and the
programming models that come with them.

One such challenge is the high cost of synchronization and
coordination across the vastly increased number of cores which might
be allocated to a single job.

Low-cost communication and synchronization between nodes is a luxury
that the HPC researcher can no longer rely on moving forward. There
are simply too many nodes! Information moving near the speed of light,
sent to issue coordinating instructions to all of the processes
running on an exascale cluster, would introduce such delays and
``bubbles'' in the computation that the simulation may as well have
been run on a present-generation petascale
machine. \todo[inline]{cite} Solutions like task-based runtime
environments (HPX, charm++, etc.)  are being developed to facilitate
general-purpose programming models that will work on both present day
petascale machines, and future exascale architectures. These
architectures forgo fixed synchronous behaviors, but also remove many
of the guarantees that formed the basis for much of the performance
models and evaluation techniques in our current paradigm.

Intelligent modern processors have been able to do branch-prediction,
out-of-order instruction execution, dynamic clock scaling, data
pre-fetch, and more, each of which introduce emergent deviations from
the a priori provable behavior characteristics of any given piece of
software. For most purposes other than optimizing low-level
infrastructure codes, this ``jitter'' was indistinguishable from
normal background noise.

Systems programmers, however, lamented this lack of predictive power
over fine-grained behavior, especially considering the potential
meaninglessness of any given observed
deviation. \todo[inline]{cite:schultz on fans/turbo mode/etc.}  This
problem only grows worse in proportion to the size and complexity of
the cluster, and the consumption of shared resources by the various
that are executing concurrently.  Variation in available power, core
clock rates, communication times, filesystem latency, and more, each
can confound the observed performance data for a program.

How can ``performance'' mean anything intelligible at all in the
exascale world, when the fine-grained code-sharpening it entailed is now
no longer observable or possibly converged at?






%the idea that traditional models of performance were 
%urces of noise are similar in concept or behavior to what is
%anticipated to play out at exascale.




There are many motivations to observe HPC application performance in
the ways described in this paper:
\begin{itemize}
\item Attribution of 
\item The ability to support orthognoal perspectives:
  \begin{itemize}
     \item Researchers may want to know how their simulation code is
       performing at various scales, document its interactions with other
       codes, and be able to profile their runs for purposes of
       reproducibility and verification of the validity of their science.
     \item System administrators want to get the most value out of the
       hardware they support, in addition to providing optimal outcomes for
       the researchers they support.
  \end{itemize}
\end{itemize}

\subsection{What is Observed}
Many factors have contributed to the emergence of variability studies
as an important research topic within the HPC community. At a
low-level, HPC node engineering has grown in complexity and
sophistication, many on-core processor behaviors that used to be
isolated, synchronous, and predictable, are now interrelated,
data-driven, asynchronous, and impossible to predict a priori. As core
density increases on the nodes of a cluster the unpredictability and
inconsistency of the hardware itself becomes an increasingly
significant contributor to observed variability. Hardware is an
important source to consider when it comes to variability because
there is almost nothing that can be done to control for it, the noise
has to be admitted in the results, and therefore is an important part
of the output of any performance-related experiment.

\subsection{How to Observe}
Performance of HPC codes can be impacted from many different sources:
\begin{itemize}
    \item Versions of software libraries across can differ across
      clusters, or even the same cluster across time.
    \item Tuning factors to extract maximum performance from a code
      can vary across clusters even if the code and the data do not
      change. Shared filesystem performance, data transport methods,
      available per-process memory, co-processor presence and
      architecture, ...and more, all can be responsible for
      influencing sensitivity to novel tuning factors.
    \item Concurrent activity elsewhere on the same cluster, activity
      that will necessarily vary between every iteration of the
      workflow, may be having a significant impact on observed
      performance.
    \item At extreme scales, parts of a simulation are almost
      guaranteed to fail due to the marginal failure rates of hardware
      components approaching absolute certainty as the number of
      involved components increases. These failures cannot be
      accurately predicted a priori, and the design constraints that
      account for and respond to them introduce performance
      perturbation and further complexity.
\end{itemize}

\subsection{SOS}
\subsection{SOSflow}
\subsection{SOSmind}



\subsubsection{Awareness of Fine-Grained Performance Jitter}
\todo[inline]{This section is purposed with showing that even episodic
  close analysis of individual components at small scale will have
  issues with significant variability. There are tractible concerns
  (demonstrating sensitivity to var.) and inherently intractible
  concerns (controlling var. across runs).}  Even in cases where
nothing can be done to influence the variations in performance, it is
important to recognize that performance variability is present and to
attempt to both quantify it and render reasonable attributions of its
source[s]. Significant variability between runs is seen (SCHULTZ
paper/graph) when tracking a single simple experiment, even if the
input data, job queue parameters, and hardware allocation is held
constant. At extreme scales, on line detection and attribution of
variability of a scientific workflow will require well-annotated
metadata to facilitate "apples to apples" comparisons driven by
unsupervised machine learning rather than a priori developer knowledge
or offline centralized analysis. It is important to characterize a
code's sensitivity to variability, as well as a cluster's propensity
for creating performance variability.

\subsubsection{Accuracy in Performance Research}
\todo[inline]{This section is intended to motivate a new model for
  performance when considering scientific workflows on exascale
  systems. Nail the coffin shut on existing performance research
  validation tools and techniques. They are intractible, per the above
  notes, and irrelevant, per this discussion} Performance research is
principally concerned with decreasing the resource consumption and
compute time required by low-level components and libraries that are
used when constructing higher-level scientific workflows. For example:
In order to validate a 5 percent increase in some code's performance,
it will be necessary to show that performance increase was observed
across a vast array of runs and hardware allocations, especially if it
is the case that a particular HPC cluster (when in an overall state
similar to the one it was in during those workflow runs) has a history
of performance variability with any statistical significance relative
to the observed performance gain. When it comes to the behavior of
codes at extreme scales, accuracy validation using traditional models
of component-based performance analysis will become cost prohibitive
in both allocation consumption and developer time.

\subsubsection{Reproduction of Experimental Results}
Scientific workflows attempt to yield results that have
truth-coorespondence with the physical world with some overt degree of
significance.

\subsection{How Variability is Tracked}
\subsubsection{Concepts and Methods}
\subsubsection{Existing Models and Tools}

\subsection{The Utility a Comprehensive Workflow Performance Model}
\subsubsection{Attribution}
\subsubsection{Resource Requirement Prediction}
\subsubsection{Automated Component Performance Tuning}
Don't want conflicting optimizer purposes. Need to know where the
hotspots *really* are and not
\subsubsection{Intelligent Compiler Hints}
Don't have to burn 1,000,000 hours of allocation to learn that you
just re-created last year's mistaken

\subsubsection{Intelligent Job Scheduling}



\section{A New Performance Model}
Traditionally, HPC performance monitoring is focused on low-level
efficiency of an application binary on some particular iron.
Higher-level systems (TACC Stats) allow the tracking and exploration
of execution wall-time for various library versions, integration of
multiple modalities of information (LDMS) like program invocation or
work allocation across a cluster as informed by network congestion
statistics, and other hybridized or meta-execution data points.
Low-level metrics are more naturally suited for off-line episodic
performance analysis of individual workflow components, but cannot
yield insight into the run-time performance of a complex workflow.
Characterizing and understanding the emergent properties of a workflow
comprised of many components that are interacting asynchronously
across a distributed HPC cluster requires taking a new approach to the
problem, especially when considering the extreme scales of parallelism
to which scientific workflows are being driven.

\subsection{Figures Revealed by Background: Invariant Meaning}

\subsection{Levels of Description and "The View from Anywhere"}
Not knowing \textit{a priori} what component or layer of the workflow
will be responsible for the introduction of variability, the workflow
performance model needs to be populated by a diversity of information
sources that provide metrics with tailored metadata and both logical
and concrete events, arriving in real-time from many different layers
of activity in a workflow:

\begin{itemize}
    \item Simulation
    \item Algorithm
    \item Application
    \item Libraries
    \item Environment
    \item Developer Tools
    \item Operating System
    \item Node Hardware
    \item Network
    \item Enclave
    \item Cluster
    \item Workflow
    \item Epoch	
\end{itemize}

Each of these layers constitutes a \textit{level of description} for
workflow performance. If the Simulation layer produced data
representing the evolution of a system over N seconds of simulated
time, and the Application layer requires M seconds of real-world
computation to yield that data, the relationship between N and M
should be a valid performance metric to report, compare across runs,
and to attempt to generally optimize through parameter convergence.

\subsection{Semantics}
All information that is gathered by the monitoring system should be
annotated as richly as possible to maximize its usefulness when
performing analytics.  Hand-annotated codes will have the most to
offer an analytics engine, values that are tracked will be able to
carry a full spectrum of high-level tags that express what that data
point means and what could be expected of it, in structure preserving
a human programmer or user's understanding while also being compatible
with unsupervised machine-learning tests for significance and other
advanced analysis techniques.

Any episodic performance measurement, such as run-time TAU
instrumentation, can also be injected into the SOSflow engine, and the
SOSflow runtime will be able to differentiate from information pushed
directly by a layer, and information that is being captured by
middle-ware tools.

\todo[inline]{Insert table heres of some of the enum values alongside
  plain-english descriptions.}

Semantic information is local to the "publication handle" (pub)
created by a source that is contributing to SOSflow.  Sources can
create multiple pubs to distinctly represent potential compound or
complex roles. Pubs carry their own pub-wide semantic markups,
including the origin layer, a role within that layer, and information
about the node that the source process is running on. Semantic markups
are then nested inside of the pub handle, as each value that is pushed
into the SOSflow system through a pub handle also comes with a rich
set of high-level semantic tags that stay affiliated with it over
time. While deep off-line data analytics can reveal unforseen
correlations between various aspects of the workflow or the data set
it is operating over, the interest in real-time analytics and
performance tuning gives value to expressing "relationship hints"
between values tracked by the system. These hints can be used to
direct in situ analytics and identify deviations from expectations;
anything that can be overtly identified as an expectation for a value
can be used to narrow the search space when doing unsupervised machine
learning over gathered workflow performance data.

\section{SOSflow}
Applying a novel semantic workflow performance model to actual
run-time environments necessitated the development of new
infrastructure software, and so the second contribution to the general
challenges of monitoring scientific workflows is the SOSflow software
artifact.

The initial design goals of SOSflow are:

\begin{enumerate}
\item Facilitate capture of scientific workflow performance data and
  events in situ (i.e. "on node") from a variety of sources and
  perspectives at the same time.
\item Annotate the gathered data's semantic meaning with expressive
  "high-level" tags that facilitate contextualization and
  introspective analytics.
\item Store the captured data on node in a way that can be searched
  with dynamic queries in real-time as well as being suitable for
  long-term centralized archival.
\end{enumerate}

\ldots the SQLite3 open-source DB engine has been selected during the
initial development phase for on-node storage, but the API is
modularized and a different format/mechanism could be used instead.
On-node storage is logically separated off-node data transport
mechanisms, but they could be the same backend if the enabling
technology supports that.

SOSflow is divided into two main parts in it's current incarnation:

\begin{itemize}
\item sosd - Daemon process running on each node
\item libsos - Library of routines for interacting with the daemon
\end{itemize}

The sosd daemon launches before a scientific workflow begins, and
passively listens on a socket.  The port that is available to all of
the sources on any given node is found in the "SOS\_CMD\_PORT"
environment variable.  Many programs and layers of programs can
connect to the daemon and send information in. Library-to-daemon
communication is invisible to the SOSflow user and happens entirely
on-node using a simple stateless protocol.  When the workflow is
completed, the sosd\_stop tool is executed on each node, it signals
the daemon to flush buffers and close down.

\subsection{Behavior}
\todo[inline]{Here I can introduce a narrative or a figure or both, to explain
  how a typical SOSflow session should happen on the cluster.}

\subsection{Implementation}
\subsubsection{Language}C
\subsubsection{External Requirements}

\begin{itemize}
    \item cmake
    \item pthreads
    \item MPI
    \item Sqlite3 (*May change across implementations)
\end{itemize}

\subsubsection{Key Source Files}
See Table 1.

\begin{table}[!t]
%% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{Key Source Files for SOSflow}
\label{table_example}
\centering
\begin{tabular}{|c|c|}
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
sos.h / sos.c & Becomes libsos, the core functions of SOSflow\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
sosd.h / sosd.c & SOSflow daemon\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
sos\_debug.h & Debugging off / on (level) knobs\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
demo\_app.c & The "Hello, world." of SOSflow value injection\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
sosd\_cloud\_mpi.c & Off-node transport using simple MPI calls\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
sosd\_db\_sqlite.c & On-node DB creation and value-injection\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{tabular}
\end{table}

\subsection{Limitations and Concerns}
The value of SOSflow is directly proportional to the quantity and
quality of the sources that are pushing semantically-annotated data
into it. At this time the use-case development has centered on the
ADIOS scalable I/O library, due to its deep integration with many
existing scientific workflows and industry standard tools like
VisIt. ADIOS instrumentation is a source of significant observable
data and an actionable target for effective feedback and control.

The more sources that are instrumented with SOSflow the less limited
the workflow performance model will be. When only one or two layers
are instrumented, the benefits of the semantic annotation and the
workflow performance model are naturally limited.

The SOSflow software artifact is in its initial development cycle and
has room for improvement in various software engineering facets:
memory subsystem interaction, diversity of storage and transport
mechanisms, and quality-of-service guarantees to both producers and
consumers of SOSflow data.

