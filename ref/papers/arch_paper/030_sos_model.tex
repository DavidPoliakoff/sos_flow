
\section{The Need to Observe}
To realize a model for performance that is suited for exascale
archictures, it is necessary to collect and analyze information as
close as possible to its source. This in situ runtime design
effectively parallelizes the execution of the observation
system and gives opportunities to cut down on coordination
overhead when providing mechanisms for on-line feedback and control.


\subsection{What is Observed}
Many factors have contributed to the emergence of variability studies
as an important research topic within the HPC community. At a
low-level, HPC node engineering has grown in complexity and
sophistication, many on-core processor behaviors that used to be
isolated, synchronous, and predictable, are now interrelated,
data-driven, asynchronous, and impossible to predict a priori. As core
density increases on the nodes of a cluster the unpredictability and
inconsistency of the hardware itself becomes an increasingly
significant contributor to observed variability. Hardware is an
important source to consider when it comes to variability because
there is almost nothing that can be done to control for it, the noise
has to be admitted in the results, and therefore is an important part
of the output of any performance-related experiment.

\subsection{How to Observe}
Performance of HPC codes can be impacted from many different sources:
\begin{itemize}
    \item Versions of software libraries across can differ across
      clusters, or even the same cluster across time.
    \item Tuning factors to extract maximum performance from a code
      can vary across clusters even if the code and the data do not
      change. Shared filesystem performance, data transport methods,
      available per-process memory, co-processor presence and
      architecture, ...and more, all can be responsible for
      influencing sensitivity to novel tuning factors.
    \item Concurrent activity elsewhere on the same cluster, activity
      that will necessarily vary between every iteration of the
      workflow, may be having a significant impact on observed
      performance.
    \item At extreme scales, parts of a simulation are almost
      guaranteed to fail due to the marginal failure rates of hardware
      components approaching absolute certainty as the number of
      involved components increases. These failures cannot be
      accurately predicted a priori, and the design constraints that
      account for and respond to them introduce performance
      perturbation and further complexity.
\end{itemize}

\subsection{SOS}
\subsection{SOSflow}
\subsection{SOSmind}



\subsubsection{Awareness of Fine-Grained Performance Jitter}
\todo[inline]{This section is purposed with showing that even episodic
  close analysis of individual components at small scale will have
  issues with significant variability. There are tractible concerns
  (demonstrating sensitivity to var.) and inherently intractible
  concerns (controlling var. across runs).}  Even in cases where
nothing can be done to influence the variations in performance, it is
important to recognize that performance variability is present and to
attempt to both quantify it and render reasonable attributions of its
source[s]. Significant variability between runs is seen (SCHULTZ
paper/graph) when tracking a single simple experiment, even if the
input data, job queue parameters, and hardware allocation is held
constant.


\subsubsection{Accuracy in Performance Research}
\todo[inline]{This section is intended to motivate a new model for
  performance when considering scientific workflows on exascale
  systems. Nail the coffin shut on existing performance research
  validation tools and techniques. They are intractible, per the above
  notes, and irrelevant, per this discussion} Performance research is
principally concerned with decreasing the resource consumption and
compute time required by low-level components and libraries that are
used when constructing higher-level scientific workflows. For example:
In order to validate a 5 percent increase in some code's performance,
it will be necessary to show that performance increase was observed
across a vast array of runs and hardware allocations, especially if it
is the case that a particular HPC cluster (when in an overall state
similar to the one it was in during those workflow runs) has a history
of performance variability with any statistical significance relative
to the observed performance gain. When it comes to the behavior of
codes at extreme scales, accuracy validation using traditional models
of component-based performance analysis will become cost prohibitive
in both allocation consumption and developer time.

\subsubsection{Reproduction of Experimental Results}
Scientific workflows attempt to yield results that have
truth-coorespondence with the physical world with some overt degree of
significance.


\subsection{The Utility a Comprehensive Workflow Performance Model}
\subsubsection{Attribution}
\subsubsection{Resource Requirement Prediction}
\subsubsection{Automated Component Performance Tuning}
Don't want conflicting optimizer purposes. Need to know where the
hotspots *really* are and not
\subsubsection{Intelligent Compiler Hints}
Don't have to burn 1,000,000 hours of allocation to learn that you
just re-created last year's mistaken
\subsubsection{Intelligent Job Scheduling}



\section{A New Performance Model}
Traditionally, HPC performance monitoring is focused on low-level
efficiency of an application binary on some particular iron.
Higher-level systems (TACC Stats) allow the tracking and exploration
of execution wall-time for various library versions, integration of
multiple modalities of information (LDMS) like program invocation or
work allocation across a cluster as informed by network congestion
statistics, and other hybridized or meta-execution data points.
Low-level metrics are more naturally suited for off-line episodic
performance analysis of individual workflow components, but cannot
yield insight into the run-time performance of a complex workflow.
Characterizing and understanding the emergent properties of a workflow
comprised of many components that are interacting asynchronously
across a distributed HPC cluster requires taking a new approach to the
problem, especially when considering the extreme scales of parallelism
to which scientific workflows are being driven.

\subsection{Figures Revealed by Background: Invariant Meaning}
So long as the input data and result output is the same, what never
changes about a simulation, nomatter how it is tuned or allocated or
configured for a run, is the meaning of what is happening.  Whether a
simulation is executing as a single process on a single node, or
billions of independent tasks in a cloud computing environment, the
meaning of what is being computed holds the same.

At extreme scales, on line detection and attribution of variability of
a scientific workflow will require well-annotated metadata to
facilitate "apples to apples" comparisons driven by unsupervised
machine learning rather than a priori developer knowledge or offline
centralized analysis. This kind of semantic invariance, in the face of
wild performance variance, is important to characterize a code's
sensitivity to performance perturbation, as well as a cluster's
propensity for creating and propagating performance variability.

\subsection{Levels of Description and "The View from Anywhere"}
Not knowing \textit{a priori} what component or layer of the workflow
will be interesting for the analysis of performance, the workflow
performance model needs to be populated by a diversity of information
sources that provide metrics with tailored metadata and both logical
and concrete events, arriving in real-time from many different layers
of activity in a workflow:
\begin{itemize}
    \item Simulation
    \item Algorithm
    \item Application
    \item Libraries
    \item Environment
    \item Developer Tools
    \item Operating System
    \item Node Hardware
    \item Network
    \item Enclave
    \item Cluster
    \item Workflow
    \item Epoch	
\end{itemize}

Each of these layers constitutes a \textit{level of description} for
workflow performance. If the Simulation layer produced data
representing the evolution of a system over N seconds of simulated
time, and the Application layer requires M seconds of real-world
computation to yield that data, the relationship between N and M
should be a valid performance metric to report, compare across runs,
and to attempt to generally optimize through parameter convergence.

\subsection{Semantics}
All information that is gathered by the monitoring system should be
annotated as richly as possible to maximize its usefulness when
performing analytics.  Hand-annotated codes will have the most to
offer an analytics engine, values that are tracked will be able to
carry a full spectrum of high-level tags that express what that data
point means and what could be expected of it, in structure preserving
a human programmer or user's understanding while also being compatible
with unsupervised machine-learning tests for significance and other
advanced analysis techniques.

Any episodic performance measurement, such as run-time TAU
instrumentation, can also be injected into the SOSflow engine, and the
SOSflow runtime will be able to differentiate from information pushed
directly by a layer, and information that is being captured by
middle-ware tools.

\todo[inline]{Insert table heres of some of the enum values alongside
  plain-english descriptions.}

Semantic information is local to the "publication handle" (pub)
created by a source that is contributing to SOSflow.  Sources can
create multiple pubs to distinctly represent potential compound or
complex roles. Pubs carry their own pub-wide semantic markups,
including the origin layer, a role within that layer, and information
about the node that the source process is running on. Semantic markups
are then nested inside of the pub handle, as each value that is pushed
into the SOSflow system through a pub handle also comes with a rich
set of high-level semantic tags that stay affiliated with it over
time. While deep off-line data analytics can reveal unforseen
correlations between various aspects of the workflow or the data set
it is operating over, the interest in real-time analytics and
performance tuning gives value to expressing "relationship hints"
between values tracked by the system. These hints can be used to
direct in situ analytics and identify deviations from expectations;
anything that can be overtly identified as an expectation for a value
can be used to narrow the search space when doing unsupervised machine
learning over gathered workflow performance data.

\section{SOSflow}
Applying a novel semantic workflow performance model to actual
run-time environments necessitated the development of new
infrastructure software, and so the second contribution to the general
challenges of monitoring scientific workflows is the SOSflow middleware.

SOSflow is designed to:
\begin{enumerate}
\item Facilitate capture of scientific workflow performance data and
  events in situ (i.e. ``on node'') from a variety of sources and
  perspectives at the same time.
\item Annotate the gathered data's semantic meaning with expressive
  ``high-level'' tags that facilitate contextualization and
  introspective analytics (SOSmind).
\item Store the captured data on node in a way that can be searched
  with dynamic queries in real-time as well as being suitable for
  long-term centralized archival.
\end{enumerate}

\ldots the SQLite3 open-source DB engine has been selected during the
initial development phase for on-node storage, but the API is
modularized and a different format/mechanism could be used instead.
On-node storage is logically separated off-node data transport
mechanisms, but they could be the same backend if the enabling
technology supports that.

SOSflow is divided into two main parts in it's current incarnation:

\begin{itemize}
\item sosd - Daemon process running on each node
\item libsos - Library of routines for interacting with the daemon
\end{itemize}

The sosd daemon launches before a scientific workflow begins, and
passively listens on a socket.  The port that is available to all of
the sources on any given node is found in the "SOS\_CMD\_PORT"
environment variable.  Many programs and layers of programs can
connect to the daemon and send information in. Library-to-daemon
communication is invisible to the SOSflow user and happens entirely
on-node using a simple stateless protocol.  When the workflow is
completed, the sosd\_stop tool is executed on each node, it signals
the daemon to flush buffers and close down.

\subsection{Behavior}
\todo[inline]{Here I can introduce a narrative or a figure or both, to explain
  how a typical SOSflow session should happen on the cluster.}

\subsection{Implementation}
\subsubsection{Language}C
\subsubsection{External Requirements}

\begin{itemize}
    \item cmake
    \item pthreads
    \item MPI
    \item Sqlite3 (*May change across implementations)
\end{itemize}

\subsubsection{Key Source Files}
See Table 1.

\begin{table}[!t]
%% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{Key Source Files for SOSflow}
\label{table_example}
\centering
\begin{tabular}{|c|c|}
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
sos.h / sos.c & Becomes libsos, the core functions of SOSflow\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
sosd.h / sosd.c & SOSflow daemon\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
sos\_debug.h & Debugging off / on (level) knobs\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
demo\_app.c & The "Hello, world." of SOSflow value injection\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
sosd\_cloud\_mpi.c & Off-node transport using simple MPI calls\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
sosd\_db\_sqlite.c & On-node DB creation and value-injection\\
\hline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{tabular}
\end{table}

\subsection{Limitations and Concerns}
The value of SOSflow is directly proportional to the quantity and
quality of the sources that are pushing semantically-annotated data
into it. At this time the use-case development has centered on the
ADIOS scalable I/O library, due to its deep integration with many
existing scientific workflows and industry standard tools like
VisIt. ADIOS instrumentation is a source of significant observable
data and an actionable target for effective feedback and control.

The more sources that are instrumented with SOSflow the less limited
the workflow performance model will be. When only one or two layers
are instrumented, the benefits of the semantic annotation and the
workflow performance model are naturally limited.

The SOSflow software artifact is in its initial development cycle and
has room for improvement in various software engineering facets:
memory subsystem interaction, diversity of storage and transport
mechanisms, and quality-of-service guarantees to both producers and
consumers of SOSflow data.

