\todofilebegin{020\_intro\_motivation.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NOTE: no \IEEEPARstart

\section{Introduction}
Modern clusters for parallel computing are complex environments and
the high-performance applications that run on them do so often with
little insight about their or the system's behavior.  This is not to
say that information is unavailable.  After all, sophisticated
parallel measurement systems can capture performance and power data
for characterization, analysis, and tuning purposes, but the
infrastructure for observation of these systems is not intended for
general use.  Rather, it is specialized for certain types of
performance information and typically does not allow online
processing.  Other sources of information of interest might include
the operating system (OS), network hardware, runtime services, or the
parallel application itself.  Cluster monitoring systems like
Ganglia \cite{massie2004ganglia} or Nagios \cite{katsaros2011building}
collect and process data about the performance and health of
cluster-wide resources, but do not provide sufficient fidelity to
capture the complex interplay between applications competing for
shared resources.  In contrast, the Lightweight Distributed Metric
Service \cite{agelastos2014lightweight} (LDMS) attempts to capture
system data continuously to obtain insight into behavioral
characteristics of individual applications with respect to their
resource utilization.  However, neither of these provides a framework
that can be configured with and used directly by the application, nor
allow for semantic encoding of multiple observation sources.

Our general interest is in parallel application monitoring: the
observation, introspection, and possible adaptation of an application
during its execution.  Application monitoring has several requirements.
Because information could come from different sources and used for
different purposes, it is important to have a flexible means for
information to be provided from both the application and the system
environment.  Because information will need to be processed online, it is
important to enable analysis in situ with the application.  Because
analysis can result in application feedback, query and control interfaces
are required, again to both the application and the system.  The problem is
that there exists no general purpose infrastructure that can be programmed,
configured, and launched with the application to provide the integrated
observation, introspection, and adaptation support required.

This paper presents the \textit{Scalable Observation System (SOS)} for
integrated application monitoring.  The SOS design emphasizes a semantic
data model with distributed information management and structured query and
access.  A dynamic database architecture is used in SOS to support
aggregation of streaming observations from multiple sources.  SOS provides
interfaces for sources of information to encode data, metadata, and
semantic context.  Interfaces are also provided for in situ analytics to
acquire information and send back results for application actuators.  SOS
launches with the application, runs along side it, and can acquire its own
resources for scalable data collection and processing.  The primary
objectives of SOS are flexibility, scalability, and programmability.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Motivation}

The designs of SOS and its semantic performance model reflect many of
the new challenges introduced by exascale HPC clusters, and the
programming models that come with them. One such challenge that is
representative of the general difficulty is the high cost of
synchronization and coordination across the vastly increased number of
cores which might be allocated to a single job.

Low-cost communication and synchronization between nodes is a luxury
that the HPC researcher can no longer rely on moving forward. There
are simply too many nodes! Information moving near the speed of light,
sent to issue coordinating instructions to all of the processes
running on an exascale cluster, would introduce such delays and
``bubbles'' in the computation that the simulation may as well have
been run on a present-generation petascale machine.  Solutions using
task-based runtime environments \cite{kaiser2014hpx} like HPX (Or
alternately: Charm++, etc.)  are being developed to facilitate
general-purpose programming models that will work on both present day
petascale machines, and future exascale architectures. These
architectures forgo fixed synchronous behaviors, but also remove many
of the guarantees that formed the basis for much of the performance
models and evaluation techniques in our current paradigm.

Intelligent modern processors have been able to do branch-prediction,
out-of-order instruction execution, dynamic clock scaling, data
pre-fetch, and more, each of which introduce emergent deviations from
the a priori provable behavior characteristics of any given piece of
software. For most purposes other than optimizing low-level
infrastructure codes, this ``jitter'' was indistinguishable from
normal background noise.

Systems programmers, however, lamented this lack of predictive power
over fine-grained behavior, especially considering the potential
meaninglessness of any given observed
deviation. \todo[inline]{cite:schultz on fans/turbo mode/etc.}  This
problem only grows worse in proportion to the size and complexity of a
cluster, and the consumption of shared resources by the various tasks
that are executing concurrently.  Differences in available power, core
clock rates, communication times, filesystem latency, and more, within
the same job, each can confound the observed performance data at any
point and overall.

But it gets worse.  Performance research is classically concerned with
decreasing the resource consumption and compute time required by
lower-level components and libraries that are used when persuing
answers to higher-level scientific questions.  When there is a lot of
noise in the information due to unpredictable interference, the only
way to validate performance increases is to perform a large number of
runs. When the variation in performance is only observable at large
scales, the tests have to be repeated at large scales, and so the
classic model for performance fails again to be useful, as code
characteristic validation becomes cost-prohibitive.

As an example of scaling validation issues with classic models,
consider that in order to validate some percentage increase in some
code's performance, it will be necessary to show that the performance
increase was observed across a large number of runs and also assorted
hardware allocations.  This is especially true when that particular
HPC cluster, even if in an overall similar state across each run, has
a history of performance variability with statistical significance
relative to the claimed performance gain. When it comes to the
behavior of codes at extreme scales, accuracy of validation tests
using traditional models of component-based performance analysis will
become cost prohibitive in both allocation consumption and developer
time.


Does ``performance'' mean anything intelligible at all in the exascale
world, when the fine-grained code-sharpening it entails is now no
longer practically observable and cannot possibly be converged on? How
do we assess the coarse-grained and state-contingent performance gains
that \textit{are} observable?

There is a clear need for a new model for performance, but even in
cases where nothing can be done to influence the fine-grained
variations in lower-level performance metrics, it remains important to
recognize and capture the fact that performance variability is present
and to \textit{attempt to both quantify it and render reasonable
attributions of its source[s]}. There are tractible concerns
(demonstrating sensitivity to variance) and inherently intractible
concerns (perfectly controlling variance across runs) that the
performance model should be designed to represent and that the runtime
middleware should be savvy to in its analytics, feedback, and control
mechanisms.

There are thus many motivations to observe HPC application performance
in the ways described in this paper:
\begin{itemize}
\item Capture metrics that represent useful facts about the exascale
runtime context, and are not merely measurements of meaningless noise.
\item Attribution of 'blame' for performance purterbation in a shared execution environment
\item Compare performance across varying implementations, HPC platforms,
      hardware allocations, parameterizations, and system loads, all in an
      ``apples to apples'' way.
\item Productively combine performance data sets to make novel correlations
      and gain understanding that was not anticipated at the time the
      codes were written or instrumented.  (Emergent knowledge requires
      some semantic framework to guide this productive combination of
      datum.)
\item The ability to provide value from a variety of perspectives with
      a common :
  \begin{itemize} \item Researchers may want to know how their
     simulation code is performing at various scales, document its
     interactions with other codes, and be able to profile their runs
     for purposes of reproducibility and verification of the validity
     of their science.  They want to maximize their consumption of
     resources, exclusively benefitting their own interests and
     compute budget.  \item System administrators want to get the most
     value out of the hardware they support, in addition to providing
     optimal outcomes for the researchers they support. They want to
     fairly share the resources between all users, and consume as
     little system resources as necessary for the administrative
     software stack to facilitate a useful and stable HPC platform for
     their users.  \end{itemize}
\end{itemize}

We now look at our proposal for this new performance model and an
implementation of it as a programmable middleware layer.


\todofileend{020\_intro\_motivation.tex}

