%\todofilebegin{020\_intro\_motivation.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NOTE: no \IEEEPARstart

%%%%%
\section{Introduction}
Modern clusters for parallel computing are complex environments and
the high-performance applications that run on them do so often with
little insight about their or the system's behavior.
%
This is not to say that information is unavailable.  After all,
sophisticated parallel measurement systems can capture performance and
power data for characterization, analysis, and tuning purposes, but
the infrastructure for observation of these systems is not intended
for general use.
%
Rather, it is specialized for certain types of performance information
and typically does not allow online processing.
%
Other information sources of interest might include the
operating system (OS), network hardware, runtime services, or the
parallel application itself.
%
Cluster monitoring systems like Ganglia \cite{massie2004ganglia} or
Nagios \cite{katsaros2011building} collect and process data about the
performance and health of cluster-wide resources, but do not provide
sufficient fidelity to capture the complex interplay between
applications competing for shared resources.
%
In contrast, the Lightweight Distributed Metric Service
\cite{agelastos2014lightweight} (LDMS) attempts to capture system data
continuously to obtain insight into behavioral characteristics of
individual applications with respect to their resource utilization.
%
However, neither of these provides a framework that can be configured
with and used directly by the application, nor allow for semantic
encoding of multiple observation sources.
%%%%%%%

%%%%%%%
Our general interest is in parallel application monitoring: the
observation, introspection, and possible adaptation of an application
during its execution.
%
Application monitoring has several requirements.  Because information
could come from different sources and be used for different purposes,
it is important to have a flexible means for information to be
provided from both the application and the system environment.
%
Because information will need to be processed online, it is important
to enable analysis in situ with the application.
%
Because analysis can result in application feedback, query and control
interfaces are required, again to both the application and the system.
%
There exists no general purpose infrastructure that can be programmed,
configured, and launched with the application to provide the
integrated observation, introspection, and adaptation support
required.
%%%%%

%%%%%
This paper presents the \textit{Scalable Observation System (SOS)} for
integrated application monitoring.
%
A working implementation of SOS is contributed as a part of this
research effort, the \textit{SOSflow} runtime.
%
The SOSflow platform demonstrates all of the essential characteristics
of the SOS model, showing the scalability and flexibility inherent to
SOS with its support for observation, introspection, feedback, and
control of scientific workflows.
%
The SOS design emphasizes a semantic data model with distributed
information management and structured query and access.
%
A dynamic database architecture is used in SOS to support aggregation
of streaming observations from multiple sources.
%
SOS provides interfaces for sources of information to encode data,
metadata, and semantic context.
%
Interfaces are also provided for in situ analytics to acquire
information and send back results for application actuators.
%
SOS launches with the application, runs along side it, and can acquire
its own resources for scalable data collection and processing.
%
The primary objectives of SOS are flexibility, scalability, and
programmability.
%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%-----------------------------------------------------------------------------

\section{Related Work}

%%%%%
Traditionally, HPC research into enhancing performance has been
focused on low-level efficiency of an application or library on some
particular mechine, with tools like TAU bringing HPC developers ever
closer to optimal runs on specific machines.
%
Low-level metrics are naturally suited for off-line episodic
performance analysis of individual workflow components.
%
Such deep instrumentation is necessarily invasive and can dictate
rather than capture the observed performance of the instrumented
application when it is run at scale or required to engage in
significant amounts of interactivity.
%
Doing principle components analysis on unconditioned data is also
computationally expensive and unsuitible for a runtime environment.
%
These tools, and many other performance monitoring tools, are
well-implemented, tested, maintained, deployed and regularly used for
performance research studies.
%%%%%

%%%%%
Higher-level systems like TACC Stats \cite{evans2014comprehensive}
allow the tracking and exploration of execution wall-time for
applications compiled using various library versions.
%
The popular Lightweight Distributed Metric System (LDMS)
\cite{agelastos2014lightweight} provides basic integration of multiple
modalities of data in real-time, triggering program invocation or
shaping work allocation across a cluster as informed by network
congestion statistics, and other hybridized or meta-execution data
points.
%
LDMS is a pull-based model, where a daemon running on nodes will
observe and store a set of values at a regular interval.
%
SOS has a hybrid push-pull model that puts users in control of the
frequency and amount of information exchanged with the runtime.
%
Further, LDMS is currently limited to working with double-precision
floating point values, while SOS allows for the collection of many
kinds of information including JSON objects and ''binary large
object'' (BLOB) data.
%%%%%

\todo[inline]{In case we want to talk about more, here are some more things.}

%%%%%
\textbf{ptrace} : One-liner about how unreasonably fine-grained this is.
\textbf{TAUg} : Being built into a low-level performance profiling tool, limits utility
\textbf{Caliper} : I probably want to talk about Caliper more.
\textbf{Cache} : I could reference Cache as a way of describing the tech I used.
\textbf{Ganglia} : ...
\textbf{Nagios} : Could have a one-liner here so people knew I knew about it.
\textbf{Pegasus} : There are commercial solutions but they are not well adapted to HPC.
%%%%%



%-----------------------------------------------------------------------------



%\todofileend{020\_intro\_motivation.tex}

